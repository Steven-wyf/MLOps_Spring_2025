#!/usr/bin/env python3
# inference/export_onnx.py

import os
import sys
import torch
import mlflow
from mlflow.tracking import MlflowClient

# â€”â€” å¦‚æœä½ çš„ MLflow Server é…ç½®äº† Artifact HTTP ä»£ç†ï¼Œç¡®ä¿å…ˆè®¾ URI â€”â€” 
mlflow.set_tracking_uri("http://129.114.25.37:8000/")

# â€”â€” ä¸ºäº†èƒ½ import ä½ ä»¬çš„è®­ç»ƒè„šæœ¬ï¼Œæ’å…¥é¡¹ç›®æ ¹åˆ° sys.path â€”â€” 
proj_root = os.path.abspath(os.path.join(__file__, "..", ".."))
if proj_root not in sys.path:
    sys.path.insert(0, proj_root)
from train.llara_train import get_latest_run_id

# 1) æ‰¾åˆ°æœ€æ–° Run
run_id = get_latest_run_id("llara-classifier")
print(f"ğŸ” Loading LLaRA model from MLflow run {run_id}")

# 2) ç”¨ PyTorch Flavor æ¥å£åŠ è½½æ¨¡å‹  
#    model_uri çš„æ ¼å¼æ˜¯ "runs:/<run_id>/<artifact_path_without_suffix>"
#    è®­ç»ƒæ—¶ä½ æ˜¯ mlflow.log_artifact(model_path, "models"),
#    é‚£ä¹ˆè¿™é‡Œå°±æ˜¯ "models/llara_model"
model_uri = f"runs:/{run_id}/models/llara_model"
model = mlflow.pytorch.load_model(model_uri)
model.eval()
print("âœ… Model loaded:", model)

# 3) ä»æ¨¡å‹ç»“æ„é‡Œè‡ªåŠ¨æ¨æ–­ input_dimï¼ˆå–ç¬¬ä¸€ä¸ª Linear å±‚çš„è¾“å…¥ç»´åº¦ï¼‰
first_linear = next(m for m in model.modules() if isinstance(m, torch.nn.Linear))
input_dim = first_linear.in_features
print(f"â„¹ï¸  Detected input_dim = {input_dim}")

# 4) å¯¼å‡ºæˆ ONNX
out_dir = os.path.abspath("models/music_rec/1")
os.makedirs(out_dir, exist_ok=True)
onnx_path = os.path.join(out_dir, "model.onnx")

# æ„é€ ä¸€ä¸ª dummy è¾“å…¥
dummy_input = torch.randn(1, input_dim)
torch.onnx.export(
    model,
    dummy_input,
    onnx_path,
    input_names=["input_embeddings"],
    output_names=["scores"],
    opset_version=13,
    dynamic_axes={
        "input_embeddings": {0: "batch"},
        "scores":           {0: "batch"},
    },
)
print(f"âœ… ONNX model saved to {onnx_path}")
