# inference/Dockerfile.staging

FROM python:3.10-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Set environment variable for MLflow
ENV MLFLOW_TRACKING_URI=http://129.114.25.37:8000

# Copy inference code (including your inference_api.py)
COPY . .

# Run FastAPI server (adjust this if your API file is named differently)
CMD ["uvicorn", "inference_api:app", "--host", "0.0.0.0", "--port", "8000"]
